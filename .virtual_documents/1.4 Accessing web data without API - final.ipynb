


# Import libraries

import pandas as pd
import time
from selenium import webdriver
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
import matplotlib.pyplot as plt
import os
import logging
import requests
import bs4


## Setup chrome options

chrome_options = Options()
chrome_options.add_argument("--headless") # Ensure GUI is off
chrome_options.add_argument("--no-sandbox")


# Install Chrome driver manager

service = Service(executable_path=ChromeDriverManager().install())


driver = webdriver.Chrome(executable_path=r'C:\Program Files\Google\Chrome\chromedriver.exe')





# Get the page's contents

page_url = "https://en.wikipedia.org/wiki/Alice%27s_Adventures_in_Wonderland"
driver.get(page_url)


# Click on Accept cookies

# time.sleep(3)
# driver.find_element(By.XPATH, '//div[text()="ACCEPT"]').click()


# Create a collection of the characters

character_elems = driver.find_elements(by = By.CLASS_NAME, value = 'div-col')


character_elems[0].text


list_char = character_elems[0].text.split("\n")


list_char


# Put the characters into a dataframe

df = pd.DataFrame(list_char, columns =  ["character"])


df


# Close the Chrome window

driver.quit()


# Save the dataframe as csv

df.to_csv('Alice_characters.csv')





# Second portion

import requests
from bs4 import BeautifulSoup


# Get URL

page = requests.get("https://en.wikipedia.org/wiki/Alice_in_Wonderland_(franchise)")


# Create soup and get title

soup = BeautifulSoup(page.text, 'html.parser')
print(soup.title)


print(soup.text)


text = soup.get_text()


text = text.encode('utf-8')


with open('Alice_article_Wiki.txt', 'wb') as f: 
    f.write(text)
